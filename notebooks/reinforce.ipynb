{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import sys, os\n",
    "import pystk\n",
    "import ray\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('device = ', device)\n",
    "ray.init(logging_level=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.actors import new_action_net, Actor, GreedyActor, SteeringActor\n",
    "from utils.utils import run_agent, rollout_many\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "action_net = new_action_net()\n",
    "data = run_agent(Actor(SteeringActor(action_net)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "many_action_nets = [new_action_net() for i in range(100)]\n",
    "\n",
    "data = rollout_many([Actor(SteeringActor(action_net)) for action_net in many_action_nets], n_steps=600)\n",
    "\n",
    "good_initialization = many_action_nets[ np.argmax([d[-1]['kart_info'].overall_distance for d in data]) ]\n",
    "bad_initialization = many_action_nets[ np.argmin([d[-1]['kart_info'].overall_distance for d in data]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = run_agent(GreedyActor(SteeringActor(bad_initialization)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall what we're trying to do in RL: maximize the expected return of a policy $\\pi$ (or in turn minmize a los $L$)\n",
    "$$\n",
    "-L = E_{\\tau \\sim P_\\pi}[R(\\tau)],\n",
    "$$\n",
    "where $\\tau = \\{s_0, a_0, s_1, a_1, \\ldots\\}$ is a trajectory of states and actions.\n",
    "The return of a trajectory is then defined as the sum of individual rewards $R(\\tau) = \\sum_k r(s_k)$ (we won't discount in this assignment).\n",
    "\n",
    "Policy gradient computes the gradient of the loss $L$ using the log-derivative trick\n",
    "$$\n",
    "\\nabla_\\pi L = -E_{\\tau \\sim P_\\pi}[\\sum_k r(s_k) \\nabla_\\pi \\sum_i \\log \\pi(a_i | s_i)].\n",
    "$$\n",
    "Since the return $r(s_k)$ only depends on action $a_i$ in the past $i < k$ we can further simplify the above equation:\n",
    "$$\n",
    "\\nabla_\\pi L = -E_{\\tau \\sim P_\\pi}\\left[\\sum_i \\left(\\nabla_\\pi \\log \\pi(a_i | s_i)\\right)\\left(\\sum_{k=i}^{i+T} r(s_k) \\right)\\right].\n",
    "$$\n",
    "We will implement an estimator for this objective below. There are a few steps that we need to follow:\n",
    "\n",
    " * The expectation $E_{\\tau \\sim P_\\pi}$ are rollouts of our policy\n",
    " * The log probability $\\log \\pi(a_i | s_i)$ uses the `Categorical.log_prob`\n",
    " * Gradient computation uses the `.backward()` function\n",
    " * The gradient $\\nabla_\\pi L$ is then used in a standard optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.reinforce import reinforce\n",
    "import copy\n",
    "\n",
    "# good_initialization = best_action_net\n",
    "action_net = copy.deepcopy(bad_initialization)\n",
    "actors = [SteeringActor(action_net)]\n",
    "best_action_net = reinforce(actors[0], actors, n_epochs=5, n_iterations=200, n_trajectories=100, n_validations=100, T=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = run_agent(GreedyActor(SteeringActor(best_action_net)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import Rollout\n",
    "viz_rollout = Rollout.remote(400, 300, track='hacienda')\n",
    "data = run_agent(GreedyActor(SteeringActor(best_action_net)), rollout=viz_rollout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
