{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '127.0.0.1',\n",
       " 'raylet_ip_address': '127.0.0.1',\n",
       " 'redis_address': None,\n",
       " 'object_store_address': '/tmp/ray/session_2022-04-24_00-32-14_714468_37765/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-04-24_00-32-14_714468_37765/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2022-04-24_00-32-14_714468_37765',\n",
       " 'metrics_export_port': 61077,\n",
       " 'gcs_address': '127.0.0.1:54943',\n",
       " 'address': '127.0.0.1:54943',\n",
       " 'node_id': '765d4e5f33433a7e6b57b7c7af5b71fae0f0f861081aef80cd81f41c'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import sys, os\n",
    "import pystk\n",
    "import ray\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('device = ', device)\n",
    "ray.init(logging_level=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.track_actors import Agent, SteeringActor, DriftActor\n",
    "from utils.utils import run_agent, rollout_many, show_trajectory_histogram\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = run_agent(Agent(SteeringActor(), train=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "many_actors = [SteeringActor() for i in range(100)]\n",
    "\n",
    "data = rollout_many([Agent(actor) for actor in many_actors], n_steps=600)\n",
    "\n",
    "good_initialization = many_actors[ np.argmax([d[-1]['kart_info'].overall_distance for d in data]) ]\n",
    "bad_initialization = many_actors[ np.argmin([d[-1]['kart_info'].overall_distance for d in data]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = run_agent(Agent(good_initialization))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall what we're trying to do in RL: maximize the expected return of a policy $\\pi$ (or in turn minmize a los $L$)\n",
    "$$\n",
    "-L = E_{\\tau \\sim P_\\pi}[R(\\tau)],\n",
    "$$\n",
    "where $\\tau = \\{s_0, a_0, s_1, a_1, \\ldots\\}$ is a trajectory of states and actions.\n",
    "The return of a trajectory is then defined as the sum of individual rewards $R(\\tau) = \\sum_k r(s_k)$ (we won't discount in this assignment).\n",
    "\n",
    "Policy gradient computes the gradient of the loss $L$ using the log-derivative trick\n",
    "$$\n",
    "\\nabla_\\pi L = -E_{\\tau \\sim P_\\pi}[\\sum_k r(s_k) \\nabla_\\pi \\sum_i \\log \\pi(a_i | s_i)].\n",
    "$$\n",
    "Since the return $r(s_k)$ only depends on action $a_i$ in the past $i < k$ we can further simplify the above equation:\n",
    "$$\n",
    "\\nabla_\\pi L = -E_{\\tau \\sim P_\\pi}\\left[\\sum_i \\left(\\nabla_\\pi \\log \\pi(a_i | s_i)\\right)\\left(\\sum_{k=i}^{i+T} r(s_k) \\right)\\right].\n",
    "$$\n",
    "We will implement an estimator for this objective below. There are a few steps that we need to follow:\n",
    "\n",
    " * The expectation $E_{\\tau \\sim P_\\pi}$ are rollouts of our policy\n",
    " * The log probability $\\log \\pi(a_i | s_i)$ uses the `Categorical.log_prob`\n",
    " * Gradient computation uses the `.backward()` function\n",
    " * The gradient $\\nabla_\\pi L$ is then used in a standard optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.reinforce import reinforce\n",
    "import copy\n",
    "\n",
    "#good_initialization = best_steering_net\n",
    "action_net = copy.deepcopy(bad_initialization.action_net)\n",
    "actors = [SteeringActor(action_net, reward_type=\"lateral\")]\n",
    "\n",
    "# iterations is high relatively here to help force a good outcome from a bad initialization\n",
    "best_steering_net = reinforce(actors[0], actors, n_epochs=5, n_iterations=100, n_trajectories=100, n_validations=100, T=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = run_agent(Agent(SteeringActor(best_steering_net)))\n",
    "data = rollout_many([Agent(SteeringActor(best_steering_net))] * 100, n_steps=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_trajectory_histogram(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = run_agent(Agent(SteeringActor(best_steering_net), DriftActor(new_action_net())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the drift action - may need to run this a few times to get a good result\n",
    "many_actors = [DriftActor() for i in range(100)]\n",
    "\n",
    "# train=True for steering to introduce a bit of uncertainty otherwise Drift may never beat the non-drift network.\n",
    "data = rollout_many([Agent(actor, SteeringActor(best_steering_net), train=True) for actor in many_actors], n_steps=600)\n",
    "\n",
    "good_initialization_drift = many_actors[ np.argmax([d[-1]['kart_info'].overall_distance for d in data]) ]\n",
    "\n",
    "actors = [SteeringActor(best_steering_net, train=False), good_initialization_drift]\n",
    "best_drift_net = reinforce(actors[1], actors, n_epochs=5, n_iterations=1000, n_trajectories=100, n_validations=20, T=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = run_agent(Agent(SteeringActor(best_steering_net), DriftActor(best_drift_net)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import Rollout\n",
    "viz_rollout = Rollout.remote(400, 300, track='hacienda')\n",
    "data = run_agent(Agent(SteeringActor(best_steering_net), DriftActor(best_drift_net)), rollout=viz_rollout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
