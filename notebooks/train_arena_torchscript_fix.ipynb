{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5f88dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%set_env TRAINING=\"True\"\n",
    "import torch\n",
    "import sys, os\n",
    "import pystk\n",
    "import ray\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('device = ', device)\n",
    "ray.init(logging_level=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc231b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from state_agent.agents.subnets.actors import SteeringActor, DriftActor, SpeedActor\n",
    "from state_agent.agents.subnets.planners import PlayerPuckGoalPlannerActor, PlayerPuckGoalFineTunedPlannerActor\n",
    "from state_agent.agents.subnets.agents import Agent, BaseTeam, Action\n",
    "from state_agent.agents.subnets.utils import Rollout, run_soccer_agent, rollout_many, show_trajectory_histogram, load_model, save_model\n",
    "from state_agent.agents.subnets.rewards import SoccerBallDistanceObjective\n",
    "from state_agent.agents.subnets.features import get_distance_cart_to_puck\n",
    "from state_agent.trainers.train_policy_gradient import reinforce, SoccerReinforcementConfiguration\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de3195c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = run_soccer_agent(Agent(SteeringActor(), train=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c969e704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initializations(actor_class):    \n",
    "    distance_objective = SoccerBallDistanceObjective(150)\n",
    "    many_actors = [actor_class() for i in range(100)]\n",
    "\n",
    "    data = rollout_many([\n",
    "        Agent(actor, accel=0.05) for actor in many_actors\n",
    "    ], randomize=True, n_steps=600)\n",
    "\n",
    "    good_initialization = many_actors[ np.argmax([distance_objective.calculate_state_score(d[-1]) for d in data]) ]\n",
    "    bad_initialization = many_actors[ np.argmin([distance_objective.calculate_state_score(d[-1]) for d in data]) ]\n",
    "    \n",
    "    return good_initialization, bad_initialization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fe3491",
   "metadata": {},
   "outputs": [],
   "source": [
    "#good_initialization = best_steering_net\n",
    "good_initialization, _ = get_initializations(SteeringActor)\n",
    "\n",
    "action_net = copy.deepcopy(good_initialization.action_net)\n",
    "actors = [SteeringActor(action_net)]\n",
    "\n",
    "def gen_agent(*args, **kwargs):\n",
    "    return Agent(*args, accel=0.05, target_speed=10.0, **kwargs)\n",
    "\n",
    "# configuration\n",
    "config = SoccerReinforcementConfiguration()\n",
    "config.agent = gen_agent\n",
    "\n",
    "# iterations is high relatively here to help force a good outcome from a bad initialization\n",
    "best_steering_net = reinforce(actors[0], actors, config, \n",
    "                              n_epochs=5, n_iterations=500, n_trajectories=200, n_validations=100, T=1\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2446e8ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = run_soccer_agent(Agent(SteeringActor(best_steering_net), accel=0.1), randomize=True, ball_location=[-6., -60.], player_location=[-20, 0, -50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8eaf9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# save the steering actor\n",
<<<<<<< HEAD
    "SteeringActor().save_model(use_jit=True)"
=======
    "SteeringActor(best_steering_net).save_model(use_jit=True)"
>>>>>>> 9b63e413acaca477146d050806dd4a0643863cf6
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a98817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Steering Actor\n",
    "def conversion(state):\n",
    "    # move net to linear.base.net\n",
    "    print(state)\n",
    "    state['linear.base.net.0.weight'] = state['net.0.weight']\n",
    "    state['linear.base.net.2.weight'] = state['net.2.weight']\n",
    "    del state['net.0.weight']\n",
    "    del state['net.2.weight']\n",
    "    return state\n",
    "\n",
    "\n",
    "best_steering_net = SteeringActor().load_model(conversion=conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834fb5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the speed actor\n",
    "good_initialization_speed, _ = get_initializations(SpeedActor)\n",
    "\n",
    "action_net = copy.deepcopy(good_initialization_speed.action_net)\n",
    "actors = [SteeringActor(best_steering_net, train=False), SpeedActor(action_net)]\n",
    "\n",
    "def gen_agent(*args, **kwargs):\n",
    "    reverse = np.random.uniform(0, 1) < 0.25\n",
    "    speed = np.random.uniform(0, 23) * (-1.0 if reverse else 1.0)\n",
    "    return Agent(*args, target_speed=speed, **kwargs)\n",
    "\n",
    "# configuration\n",
    "config = SoccerReinforcementConfiguration()\n",
    "config.agent = gen_agent\n",
    "\n",
    "# iterations is high relatively here to help force a good outcome from a bad initialization\n",
    "best_speed_net = reinforce(actors[1], actors, config, \n",
    "                              n_epochs=5, n_iterations=50, n_trajectories=200, n_validations=100, T=1\n",
    "                    )\n",
    "best_speed_net = action_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee85cd6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = run_soccer_agent(Agent(SteeringActor(best_steering_net), SpeedActor(best_speed_net), target_speed=4.5), randomize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0475f3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best_speed_net\n",
    "speed_actor = SpeedActor(best_speed_net)\n",
    "speed_actor.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841e862c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load the Speed Actor\n",
    "best_speed_net = SpeedActor().load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3de409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the drift actor\n",
    "#good_initialization_drift, _ = get_initializations(DriftActor)\n",
    "\n",
    "#action_net = copy.deepcopy(good_initialization_drift.action_net)\n",
    "actors = [SteeringActor(best_steering_net, train=False), DriftActor(action_net), SpeedActor(best_speed_net, train=False)]\n",
    "\n",
    "def gen_agent(*args, **kwargs):\n",
    "    speed = np.random.uniform(0, 23)\n",
    "    return Agent(*args, target_speed=speed, **kwargs)\n",
    "\n",
    "# configuration\n",
    "config = SoccerReinforcementConfiguration()\n",
    "config.agent = gen_agent\n",
    "\n",
    "best_drift_net = reinforce(actors[1], actors, config, \n",
    "                              n_epochs=5, n_iterations=200, n_trajectories=1000, n_validations=100, T=2\n",
    "                    )\n",
    "best_drift_net = action_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4a336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the drift actor\n",
    "DriftActor(best_drift_net).save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778b10d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the drift actor\n",
    "best_drift_net=DriftActor().load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8481da38",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = run_soccer_agent(Agent( \n",
    "    SteeringActor(best_steering_net),\n",
    "    DriftActor(best_drift_net),\n",
    "    SpeedActor(best_speed_net),        \n",
    "    target_speed=17.0\n",
    "), randomize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81ec216",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train the player goal scoring planner\n",
    "\n",
    "def create_planner_actor():\n",
    "    return PlayerPuckGoalPlannerActor()\n",
    "\n",
    "def gen_agent(*args, **kwargs):\n",
    "    return Agent(*args, target_speed=np.random.uniform(0, 23), **kwargs)\n",
    "\n",
    "def rollout_initializer(world_info, randomize, **kwargs):\n",
    "        \n",
    "    wall_case = np.random.uniform(low=0.0, high=1.0) < 0.1\n",
<<<<<<< HEAD
    "    #wall_case = True\n",
=======
    "    wall_case = True\n",
>>>>>>> 9b63e413acaca477146d050806dd4a0643863cf6
    "    \n",
    "    # generate a rollout where the player and puck are near each other\n",
    "    position = [\n",
    "        np.random.uniform(low=-35, high=35),\n",
    "        np.random.uniform(low=-50, high=62), # trying to force the puck to a 90 degree angle at worst (near the wall)\n",
    "    ]\n",
    "    \n",
    "    offset = [np.random.uniform(low=-1.0, high=1.0), -6]    \n",
    "        \n",
    "    if wall_case:\n",
    "        side = np.sign(np.random.uniform(-1, 1))\n",
    "        #player_location = [20 * side, 1, np.random.uniform(60, 62)]\n",
    "        player_location = None\n",
    "        position = ball_location=[45 * side, -40]\n",
    "    else:        \n",
    "        player_location = [position[0] + offset[0], 1, position[1] + offset[1]]\n",
    "        \n",
    "    world_info.set_ball_location((position[0], 1, position[1]), (0, 0, 0))        \n",
    "    if player_location is not None:\n",
    "        world_info.set_kart_location(0, player_location, [0, 0, 0, 1.0], 0)\n",
    "        \n",
    "def post_epoch(actor, context):\n",
    "    # show a histogram of distances\n",
    "    show_trajectory_histogram(context.trajectories, get_distance_cart_to_puck, max=60, bins=20)\n",
    "    plt.hist(context.rewards) \n",
    "    plt.title(\"Rewards\")\n",
    "    plt.show()\n",
    "    plt.hist(context.actions, 3, range=(0, 3)) \n",
    "    plt.title(\"Actions\")\n",
    "    plt.show()\n",
    "    print(np.sum(np.argmax(context.actions, axis=1) == 0), np.sum(np.argmax(context.actions, axis=1) == 1), np.sum(np.argmax(context.actions, axis=1) == 2))\n",
    "\n",
    "#good_initialization_planner, _ = get_initializations(create_planner_actor)\n",
    "\n",
    "# Going to need to train this a few times as the convergence is slow!!! Comment this line to retrain \n",
    "#action_net = copy.deepcopy(good_initialization_planner.action_net)\n",
    "action_net = best_planner_net\n",
    "actors = [PlayerPuckGoalPlannerActor(action_net), SpeedActor(best_speed_net, train=False), SteeringActor(best_steering_net, train=False)]\n",
    "\n",
    "# give it a positive random weight to make it the worst case\n",
    "#action_net.net[0].weight = torch.nn.Parameter(torch.Tensor([[np.random.uniform(0, 1.0)]]))\n",
    "\n",
    "#starting_weight = action_net.net[0].weight.clone()\n",
    "#print(\"Starting weight\", action_net.net[0].weight)\n",
    "\n",
    "# configuration\n",
    "config = SoccerReinforcementConfiguration()\n",
    "config.agent = gen_agent\n",
    "config.rollout_initializer = rollout_initializer\n",
    "\n",
    "# iterations is high relatively here to help force a good outcome from a bad initialization\n",
    "best_planner_net = reinforce(actors[0], actors, config, \n",
    "                              n_epochs=4, n_iterations=200, n_trajectories=500, n_validations=20, T=1,\n",
    "                              epoch_post_process=post_epoch\n",
    "                    )\n",
    "best_planner_net = action_net\n",
    "\n",
    "#assert(action_net.net[0].weight != starting_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb70ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# case 1 against a wall\n",
    "#assert(action_net(torch.Tensor([10.255, 0.9, 0.0, 0.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0]))[0] == 1)\n",
    "# case 2, travelling to puck\n",
    "#assert(action_net(torch.Tensor([10.255, 10.255, 10.01, 10.01, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0]))[0] == 2)\n",
    "# case 3, moving puck\n",
    "#assert(action_net(torch.Tensor([-1.03, -0.24, 2.0, -0.01, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0]))[0] == 2)\n",
    "\n",
    "\n",
    "action_net = best_planner_net\n",
    "action_net.train()\n",
    "output = action_net(torch.Tensor([-0.055, -0.01, 0.12, 0.12, -0.35, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0]))\n",
    "output1 = action_net(torch.Tensor([-0.055, -0.1, 0.13, 0.12, 0.4, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0]))\n",
    "output2 = action_net(torch.Tensor([10.255, 0.9, 0.2, 0.4, -0.2, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0]))\n",
    "output3 = action_net(torch.Tensor([10.255, 0.9, 0.1, 0.1, 0.2, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0]))\n",
    "\n",
    "output4 = action_net(torch.Tensor([10.255, 0.9, 2.0, 10.1, 10.01, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0]))\n",
    "output5 = action_net(torch.Tensor([-0.01, -0.01, 2.0, 12.1, 0.01,  1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0]))\n",
    "action_net.eval()\n",
    "print(output)\n",
    "print(output1)\n",
    "print(output2)\n",
    "print(output3)\n",
    "print(output4)\n",
    "print(output5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dea95f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = run_soccer_agent(Agent( \n",
    "    PlayerPuckGoalPlannerActor(        \n",
    "        action_net=best_planner_net       \n",
    "    ), \n",
    "    SteeringActor(best_steering_net),\n",
    "    SpeedActor(best_speed_net),            \n",
    "    DriftActor(best_drift_net),\n",
    "    accel=0.1, target_speed=12.0\n",
    "), randomize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dfada2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Save the Planner Actor\n",
    "PlayerPuckGoalPlannerActor(best_planner_net).save_model(use_jit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e979b283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the planner actor\n",
    "best_planner_net=PlayerPuckGoalPlannerActor().load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3851efde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train the fine tuned planner\n",
    "\n",
    "def create_planner_actor():\n",
<<<<<<< HEAD
    "    return PlayerPuckGoalFineTunedPlannerActor()\n",
=======
    "    return PlayerPuckGoalFineTunedPlannerActor(mode=\"speed\")\n",
>>>>>>> 9b63e413acaca477146d050806dd4a0643863cf6
    "        \n",
    "\n",
    "def gen_agent(*args, **kwargs):\n",
    "    return Agent(*args, **kwargs)\n",
    "\n",
    "def post_epoch(actor, context):\n",
    "    # show a histogram of distances\n",
    "    show_trajectory_histogram(context.trajectories, get_distance_cart_to_puck, max=60, bins=20)\n",
    "    plt.hist(context.rewards) \n",
    "    plt.title(\"Rewards\")\n",
    "    plt.show()\n",
    "    plt.hist(context.actions, 3, range=(0, 3)) \n",
    "    plt.title(\"Actions\")\n",
    "    plt.show()\n",
    "    \n",
    "    # extract speed offsets\n",
    "    plt.hist(np.array(context.outputs)[:,PlayerPuckGoalFineTunedPlannerActor.OUT_FEATURE_SPEED_OFFSET], 50, range=(-23, 23)) \n",
    "    plt.title(\"Speeds\")\n",
    "    plt.show()\n",
    "    print(np.sum(np.argmax(context.actions, axis=1) == 0), np.sum(np.argmax(context.actions, axis=1) == 1), np.sum(np.argmax(context.actions, axis=1) == 2))\n",
    "\n",
    "def rollout_initializer(world_info, randomize, **kwargs):\n",
    "        \n",
    "    wall_case = np.random.uniform(0, 1.0) < 0.1\n",
    "    #wall_case = True\n",
    "    \n",
    "    # generate a rollout where the player and puck are near each other\n",
<<<<<<< HEAD
    "    position = np.random.uniform(low=-30, high=30, size=(2))\n",
    "    offset = [np.random.uniform(-30, 30), np.random.uniform(-15, 0)]    \n",
=======
    "    position = np.random.uniform(low=-20, high=20, size=(2))\n",
    "    offset = [np.random.uniform(-20, 20), np.random.uniform(-15, 0)]    \n",
>>>>>>> 9b63e413acaca477146d050806dd4a0643863cf6
    "    world_info.set_ball_location((position[0], 1, position[1]), (0, 0, 0))        \n",
    "    \n",
    "    if wall_case:\n",
    "        player_location = [20 * np.sign(np.random.uniform(-1, 1)), 1, 62]\n",
    "    else:        \n",
    "        player_location = [position[0] + offset[0], 1, position[1] + offset[1]]\n",
    "    world_info.set_kart_location(0, player_location, [0, 0, 0, 1.0], 0)\n",
    "    \n",
    "#good_initialization_planner, _ = get_initializations(create_planner_actor)\n",
    "\n",
    "#action_net = copy.deepcopy(good_initialization_planner.action_net)\n",
    "actors = [\n",
    "    PlayerPuckGoalPlannerActor(best_planner_net, train=False),\n",
<<<<<<< HEAD
    "    PlayerPuckGoalFineTunedPlannerActor(action_net),\n",
=======
    "    PlayerPuckGoalFineTunedPlannerActor(action_net, mode=\"speed\"),\n",
    "    DriftActor(best_drift_net, train=False),\n",
>>>>>>> 9b63e413acaca477146d050806dd4a0643863cf6
    "    SpeedActor(best_speed_net, train=False),\n",
    "    SteeringActor(best_steering_net, train=False)\n",
    "]\n",
    "\n",
    "# give it a positive random weight to make it the worst case\n",
    "#action_net.net[0].weight = torch.nn.Parameter(torch.Tensor([[np.random.uniform(0, 1.0)]]))\n",
    "\n",
    "#starting_weight = action_net.net[0].weight.clone()\n",
    "#print(\"Starting weight\", action_net.net[0].weight)\n",
    "\n",
    "# configuration\n",
    "config = SoccerReinforcementConfiguration()\n",
    "config.agent = gen_agent\n",
    "config.rollout_initializer = rollout_initializer\n",
    "\n",
    "# iterations is high relatively here to help force a good outcome from a bad initialization\n",
    "best_planner_ft_net = reinforce(actors[1], actors, config, \n",
<<<<<<< HEAD
    "                              n_epochs=4, n_iterations=200, n_trajectories=200, n_validations=20, T=10,\n",
=======
    "                              n_epochs=4, n_iterations=200, n_trajectories=200, n_validations=20, T=[2, 3, 4, 5, 7, 10, 15],\n",
>>>>>>> 9b63e413acaca477146d050806dd4a0643863cf6
    "                              epoch_post_process=post_epoch\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95de455a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#viz_rollout_soccer = Rollout.remote(400, 300, mode=\"soccer\", players=[(0, False, \"tux\")], num_karts=1)\n",
    "data = run_soccer_agent(Agent( \n",
    "    PlayerPuckGoalPlannerActor(\n",
    "        action_net=best_planner_net        \n",
    "    ),        \n",
    "    PlayerPuckGoalFineTunedPlannerActor(                \n",
    "        best_planner_ft_net,\n",
    "        mode=\"speed\"\n",
    "    ), \n",
<<<<<<< HEAD
    "    SpeedActor(best_speed_net),\n",
    "    SteeringActor(best_steering_net), \n",
    "), randomize=True, player_location=[20, 1, 62]) #rollout=viz_rollout_soccer)"
=======
    "    DriftActor(best_drift_net),\n",
    "    SpeedActor(best_speed_net),\n",
    "    SteeringActor(best_steering_net), \n",
    "), randomize=True) #rollout=viz_rollout_soccer)"
>>>>>>> 9b63e413acaca477146d050806dd4a0643863cf6
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4551155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Planner Actor\n",
    "PlayerPuckGoalFineTunedPlannerActor(best_planner_ft_net).save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde201b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Planner Actor \n",
    "best_planner_ft_net2 = PlayerPuckGoalFineTunedPlannerActor().load_model()\n",
    "action_net = best_planner_ft_net2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bc5d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_rollout_soccer = Rollout.remote(400, 300, mode=\"soccer\", players=[(0, False, \"tux\")], num_karts=2)\n",
    "data = run_soccer_agent(Agent( \n",
    "    PlayerPuckGoalPlannerActor(\n",
    "        action_net=best_planner_net\n",
    "    ),\n",
    "    PlayerPuckGoalFineTunedPlannerActor(                \n",
    "        best_planner_ft_net,\n",
    "        mode=\"speed\"\n",
    "    ), \n",
    "    SteeringActor(best_steering_net),\n",
    "    DriftActor(best_drift_net),\n",
    "    SpeedActor(best_speed_net),\n",
    "    target_speed=15.0\n",
    "), rollout=viz_rollout_soccer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad426346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_loop(optim, net):\n",
    "    loss_bce = torch.nn.BCEWithLogitsLoss()\n",
    "    net.train()\n",
    "    optim.zero_grad()\n",
    "    idx = torch.randperm(batch_features.shape[0])\n",
    "    t_shuffled = batch_features[idx]\n",
    "    batch_actions, batch_rewards, batch_labels = reward(t_shuffled)\n",
    "    output = (net(t_shuffled)).squeeze() # + 1) / 2\n",
    "    #print(output)\n",
    "    #log_p = log_prob(output, actions=batch_actions)\n",
    "    #expected_log_return = (log_p.squeeze()*batch_returns).mean()\n",
    "    loss = loss_bce(output, batch_labels)    \n",
    "    #(-expected_log_return).backward()\n",
    "    loss.backward()\n",
    "\n",
    "    #print(\"Grad\", net.net[0].weight.grad)\n",
    "\n",
    "    optim.step()\n",
    "    net.eval()\n",
    "    return batch_actions\n",
    "\n",
    "def nll_loop(optim, net, static_reward=True):\n",
    "    net.train()\n",
    "    optim.zero_grad()\n",
    "    idx = torch.randperm(batch_features.shape[0])\n",
    "    t_shuffled = batch_features[idx]\n",
    "    batch_actions, batch_rewards, batch_labels = reward(t_shuffled)\n",
    "    output = (net(t_shuffled)).squeeze() # + 1) / 2\n",
    "    #print(output)    \n",
    "    if static_reward:\n",
    "        log_p = log_prob(output, actions=batch_labels)\n",
    "        expected_log_return = (log_p.squeeze()*1).mean()\n",
    "    else:\n",
    "        log_p = log_prob(output, actions=batch_actions)\n",
    "        expected_log_return = (log_p.squeeze()*batch_rewards).mean()\n",
    "    (-expected_log_return).backward()\n",
    "    #loss.backward()\n",
    "\n",
    "    #print(\"Grad\", net.net[0].weight.grad)\n",
    "\n",
    "    optim.step()\n",
    "    net.eval()\n",
    "    return batch_actions\n",
    "\n",
    "    \n",
    "net=LinearWithSigmoid(n_inputs=2, n_hidden=10, n_outputs=1, bias=True)\n",
    "optim = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "print(\"bce (before)\", ((net(batch_features) > 0.5).squeeze() == correct_labels).sum())\n",
    "data = [bce_loop(optim, net) for i in range(10000)]\n",
    "print(\"bce\", ((net(batch_features) > 0.9).squeeze() == correct_labels).sum())\n",
    "\n",
    "net=LinearWithSigmoid(n_inputs=2, n_hidden=10, n_outputs=1, bias=True)\n",
    "optim = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "print(\"nll (before)\", ((net(batch_features) > 0.5).squeeze() == correct_labels).sum())\n",
    "data = [nll_loop(optim, net, static_reward=True) for i in range(10000)]\n",
    "print(\"nll\", ((net(batch_features) > 0.9).squeeze() == correct_labels).sum())\n",
    "\n",
    "net=LinearWithSigmoid(n_inputs=2, n_hidden=10, n_outputs=1, bias=True)\n",
    "optim = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "print(\"nll with rewards (before)\", ((net(batch_features) > 0.5).squeeze() == correct_labels).sum())\n",
    "data = [nll_loop(optim, net, static_reward=False) for i in range(10000)]\n",
    "print(\"nll with rewards\", ((net(batch_features) > 0.9).squeeze() == correct_labels).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117e3039",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([[12.0, 102], [20, 39]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2dde0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5f000776b47a127987c20a83a3dfb0cfe02f22fbf239a7d3113e575b6e6c7143"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
